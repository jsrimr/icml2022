\section{Preliminaries and Related Work}
\textbf{Background.} We consider a Markov Decision Process (MDP) $\mathcal{M}=(\mathcal{S}, \mathcal{A}, p)$ without an external reward function for a typical unsupervised reinforcement learning. $\mathcal{S}$ is a state space, $\mathcal{A}$ is an action space, and $p(s'| s, a)$ is a dynamics. Given a skill $z$ sampling from a skill distribution $p(z)$, a policy $\pi(a | s, z)$ is learned in option framework.


\textbf{Skill discovery.} In unsupervised RL setting, \emph{skill discovery} is indicated to learn diverse behaviors with an intrinsic reward such as \emph{empowerment} \cite{salge2014empowerment}. This is formulated by maximizing mutual information between states and skills $\mathcal{I}(S; Z) = \mathcal{H}(S) - \mathcal{H}(S; Z) = \mathcal{H}(Z) - \mathcal{H}(Z; S)$, where $\mathcal{H}(\cdot)$ is the entropy in information theory. There are a number of previous methods to optimize the objective above. VIC \cite{gregor2016variational} and DIAYN \cite{eysenbach2018diversity} considers the final state and every state, respectively, in order to disciminate each skill. HIDIO \cite{zhang2021hierarchical} learns high level policy choosing skills in a hierarchical manner. APS \cite{liu2021aps} leverages successor feature to learn a diverse bahaviors. DADS \cite{sharma2019dynamics} focuses on the predictability of behaviors and learn dynamics. 

% SMM \cite{} and CIC \cite{} encourage agent to explore the state space while learning skills.

After the skill discovery phase, an agent should adapt to the task with learned skills. A simple but general method is to select one skill with initially highest reward for each task and further fine-tune this skill \cite{eysenbach2018diversity, laskin2022cic}. Some methods train an additional high level policy to choose skills to utilize more than just one skill for each task in a sequential way \cite{sharma2019dynamics}. For a specific purposed task such as a goal-reaching, there are zero shot skill adaptation methods \cite{sharma2019dynamics, choi2021variational}. We propose the general purposed but effective fine-tuning method for which a few additional parameters are needed to combine the skills.

TODO: Add DIAYN formulation (?)