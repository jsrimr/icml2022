\section{Introduction} expert data to train a state encoder through an auxiliary classifier, which tries to distinguish expert-visited states While autonomous learning of diverse and complex behavfrom random states. We then use the encoder to project iors is challenging, significant progress has been made using the state space into a latent embedding that preserves infordeep reinforcement learning (DRL). The progress has been mation that makes expert-visited states recognizable. This method extends readily to other mechanisms of learned stateaccelerated by the powerful representational learning of deep neural networks (LeCun et al. 2015 ), and the scalaprojections and different skill discovery algorithms. Crubility and efficiency of RL algorithms (Mnih et al. 2015 . cially, our method requires only samples of expert-visited Schulman et al. 2017: Haarnoja et al. 2018 . Lillicrap et al. ). states, which can easily be obtained from any reference However, DRL still involves an externally designed reward policy, for example expert demonstrations. function that guides learning and exploration. Manually The key contribution of this paper is a simple method engineering such a reward function is a complex task that for learning a parameterized state projection that guides requires significant domain knowledge in such a way that skill discovery towards a substructure of the observation hinders autonomy and adoption of RL. Prior works have prospace. We demonstrate the flexibility of our state-projection posed using unsupervised skill discovery to alleviate these method and how it can be used with the skill-discovery challenges by using empowerment as an intrinsic motivation objective. We also present empirical results that show the to explore and acquire abilities (Salge et al. 2014, Gregor performance of our method in various locomotion tasks. ${ }^{*}$ Equal contribution ${ }^{1}$ Department of Computer Science, Norwegian University of Science and Technology ${ }^{2}$ Department of Information Security and Communication Technology, Norwegian

\section{Related Work} University of Science and Technology. Correspondence to: Even Klemsdal <even.klemsdal@ntnu.no> Unsupervised reinforcement learning aims at learning diverse behaviours in a task-agnostic fashion without guidance Accepted by the ICML 2021 workshop on Unsupervised Reinforcement Learning, PMLR 139,2021 . Copyright 2021 by the from an extrinsic reward function (Jaderberg et al. 2016). author(s). This can be accomplished through learning with an intrin- sic reward such as curiosity (Oudeyer \& Kaplan 2009 or tion: $\mathcal{M}=(\mathcal{S}, \mathcal{A}, \mathcal{P})$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the empowerment (Salge et al. 2014). The notion of curiosity action space, and $\mathcal{P}: \mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow[0, \infty)$ is the tranhas been utilized for exploration by using predictive models sition probability density function. The RL agent learns of the observation space and providing a higher intrinsic a skill-conditioned policy $\pi_{\vartheta}(a \mid s, z)$, where the skill $z$ is reward for visiting unexplored trajectories (Pathak et al. sampled from some distribution $p(z)$. A skill, or option 2017). Empowerment addresses maximizing an agent's con(as first introduced in (Sutton \& Barto, 2018), is a temtrol over the environment by exploring states with maximal poral abstraction of a course of actions that extends over intrinsic options (skills). many time steps. We will also consider the informationtheoretic notion of mutual information between states and Several approaches have been proposed in the literature to skills $\mathcal{I}(S ; Z)=\mathcal{H}(Z)-\mathcal{H}(Z \mid S)$, where $\mathcal{H}(\cdot)$ is the Shanutilize empowerment for skill discovery in unsupervised RL. non entropy. Gregor et al. Gregor et al. 2016 developed an algorithm that learns intrinsic skill embedding and used generalization to discover new goals. They used the mutual information

\subsection{Skill Discovery Objective} between skills and final states as the training objective and The overall goal of skill discovery is to find a policy $\pi_{\vartheta}$ cahence used a discriminator to distinguish between different pable of carrying out different tasks that are learned without skills. Eysenbach et al. (Eysenbach et al. 2018) used mutual information between skills and states as an objective extrinsic supervision for each type of behavior. We conwhile using a fixed embedding distribution of skills. Adsider policies of the form $\pi_{\vartheta}(a \mid s, z)$ that specify different distributions over actions depending on which skill $z$ they ditionally, they used a maximum-entropy policy (Haarnoja are conditioned on. Although this general framework does et al. 2018 to produce stochastic skills. However, most of the previous approaches assume a state distribution induced not constrain how $z$ should be represented, we define it as a by the policy itself, resulting in a premature commitment discrete variable since it has been empirically shown to perform better than continuous alternatives (Eysenbach et al. to already discovered skills. Campos et al. Campos et al. 2018 . 2020 used a fixed uniform distribution over states to break the dependency between the state distribution and the policy. We follow the framework proposed by the "diversity is all you need" (DIAYN) algorithm (Eysenbach et al. 2018), Certain prior work has addressed the challenge of complex and high dimensional state space by constraining the in which skills are learned by defining an intrinsic reward that promotes diversity. Intuitively, each skill should make skill-discovery in a subset of the state space. Sharma et al. the agent visit a unique section of the state space. This (Sharma et al. 2019 l learned predictable skills by training a can be expressed as maximising the mutual information skill-conditioned dynamic model instead of a discriminator to model specific behaviour in a subset of the state space. $\mathcal{I}(S ; Z)$ of the state visitation distributions for different Eysenbach et al. (Eysenbach et al. 2018 ) proposed incorskills (Salge et al. 2014). To ensure that the visited areas of the state space are spaced sufficiently far apart, we use a soft porating prior knowledge by conditioning the discriminator policy that maximises the entropy of the action distribution. on a subset of the state space using a hand-crafted and a task-specific transformation. Our work addresses this chalFormally, we maximize the following objective function: lenge by guiding the skill discovery towards the subset of expert-visited states. In contrast to inverse reinforcement learning, (Fu et al. 2018 , we do not explicitly infer the extrinsic reward. Crucially, we do not try to learn the expert policy directly in contrast to behaviour cloning or imitation

$$
\begin{aligned}
\mathcal{F}(\theta) & \triangleq I(S ; Z)+\mathcal{H}(A \mid S)-I(A ; Z \mid S) \\
&=\mathcal{H}[A \mid S, Z]-\mathcal{H}[Z \mid S]+\mathcal{H}[Z]
\end{aligned}
$$
learning 2011 . Our proposed method resembles the algorithm proposed by Li et al. (Li et al. 2020 ) in which they used a Bayesian classifier that estimates the probability of successful outcome states, resulting in a more The first term $\mathcal{H}[A \mid S, Z]$ means that the policy should act as task-directed exploration. However, their algorithm does randomly as possible and can be optimized by maximizing not optimize the mutual information; hence it does not learn the policy's entropy. The second term $-\mathcal{H}[Z \mid S]$ dictates diverse skills via the discriminability objective. that each visited state should (ideally) identify the current skill. The third term is the entropy of the skill distribution, which can be maximized by deliberately sampling skills

\section{Preliminaries} from a uniform distribution during training. Unfortunately, $-\mathcal{H}[Z \mid S]$ requires knowledge about $p(z \mid s)$, which is not In this paper, we formalize the problem of skill discovery as readily available. Consequently, we approximate the true a Markov decision process (MDP) without a reward funcdistribution by training a classifier $q_{\phi}(z \mid s)$, leading to a 

$$
\begin{aligned}
&\text { lower bound: } \\
&\begin{aligned}
\mathcal{F}(\theta) &=\mathcal{H}[A \mid S, Z]+\mathbb{E}_{p}[\log p(z \mid s)]-\mathbb{E}_{p}[\log p(z)] \\
& \geq \mathcal{H}[A \mid S, Z]+\mathbb{E}_{p}\left[\log q_{\phi}(z \mid s)\right]-\mathbb{E}_{p}[\log p(z)]
\end{aligned}
\end{aligned}
$$

The lower bound follows from the non-negative property of the Kullback-Leibler divergence $D_{K L}(p \| q)=$ $\mathbb{E}_{p}[\log p(z \mid s)-\log q(z \mid s)] \geq 0$, which can be rearranged to $\mathbb{E}_{p}[\log p(z \mid s)] \geq \mathbb{E}_{p}[\log q(z \mid s)]$ Agakov 2004$)$.

The classifier $q_{\phi}$ is fitted throughout training with maximum likelihood estimation over the sampled states and active skills. This leads to a scenario where the policy is rolled out for a (uniformly) sampled skill, and the classifier is trained to detect the skill based on the states that were visited. The policy is given a reward proportional to how well the classifier could detect the skill in each state. In the end, this should make the policy favor visiting disjoint sets of states for each skill, leading to a cooperative game between $q_{\phi}$ and $\pi_{\vartheta}$.

improve the skills discovered for a navigation task, we aim to learn a parameterized $f_{\chi}$ by using expert data.

\subsection{Limitations of existing methods}

A major challenge that arises when maximizing the objec-

\subsection{State Space Projections} tive in Equation 2 particularly in applications with highdimensional spaces, is that it becomes trivial for each skill

We consider linear projections of continuous factored state representations on the form $f_{\chi}: \mathbb{R}^{|S|} \rightarrow \mathbb{R}^{|E|}$ with $e=$ to find a sub-region of the state space where it is easy to be recognised by $q_{\phi}$. In preliminary experiments, we ob$f_{\chi}(s)=\chi s, \chi \in \mathbb{R}^{|E| \times|S|}$, and $|E|<|S|$. In principle, the idea should apply to more complex mappings, such as a served that the existing methods discovered behaviours that multi-layer perceptron. However, we want to limit the scope covered small parts of the state space. For the HalfCheetah of skill discovery to a hyperplane within the original state environment (Brockman et al. 2016) this resulted in many space. skills generating different types of static poses (see Figure 1 and not many skills exhibiting "interesting" behaviour

For the same reason, we also omit any non-linearities in the such as locomotion. encoder. Squeezing the output through a Sigmoidal function would limit discriminability at the (potentially interesting)

Optimising for $\mathcal{H}[A \mid S, Z]$ should mitigate this issue to some extent. Increasing the policy's entropy incentivises the skills extremes of the encoding. Similarly, a ReLU function would effectively eliminate all exploration along the negative dito progressively visit regions of the state space that are so rection of $f_{\chi_{i}}$. In summary, the objective of the DIAYN far apart that not even highly stochastic actions will cause skill classifier becomes: them to overlap accidentally. However, it has been shown that mutual information based algorithms have difficulties spreading out to novel states due to low values of $\log q_{\phi}(z \mid s)$

$$
\max _{\phi} \mathbb{E}\left[\log q_{\phi}(z \mid e)\right]
$$
for out-of-sample states (Campos et al. 2020 .

\section{Proposed Method}

We learn the parameters $\chi$ for the projection through an auxiliary discriminative objective. Specifically, a binary The main idea of our approach is to focus the skill discovclassifier $h_{\psi}: \mathbb{R}^{|\mathbb{E}|} \rightarrow\{0,1\}$ is trained to predict whether ery towards certain parts of the state space by using expert an (encoded) state was sampled from the marginal state data as a prior. The DIAYN algorithm can be biased tovisitation distribution of a random policy $\pi_{\text {rand }}$ or from the wards a user-specified part of the state space by changing distribution of a reference (expert) policy $\pi^{*}$. Let $x \sim \mathcal{D}$ the discriminator to maximize $\mathbb{E}\left[\log q_{\phi}(z \mid f(s))\right]$, where $f$ denote whether a state $s$ was visited by the reference policy represents some transformation of the state space (Eysenor not (in dataset $\mathcal{D}$ ), then the parameters of $f_{\chi}$ are obtained bach et al. 2018 . Instead of using a hand-crafted $f$ to through joint pretraining with $h_{\psi}$ by maximizing the log 

$$
\begin{aligned}
&\text { likelihood over } \mathcal{D} \text { : } \\
&\max _{\chi, \psi} \mathbb{E}_{x, s \sim \mathcal{D}}\left[x \log h_{\psi}(x \mid e)+(1-x) \log \left(1-h_{\psi}(x \mid e)\right)\right]
\end{aligned}
$$

same quantity. This allows us to sample differentiable actions and climb the gradient of the minimum of the two Q-functions (DDPG-style update (Lillicrap et al.)), giving us this objective:

$$
J\left(\pi_{\vartheta}\right)=\min _{i \in\{1,2\}} Q_{\theta}^{i}\left(\pi_{\vartheta}(a \mid s)\right)+\alpha \mathcal{H}\left(\pi_{\vartheta}(a \mid s)\right)
$$

where the dataset $\mathcal{D}$ is collected prior to training the main RL algorithm. The first half (random samples) are collected Like in DIAYN, we also use a Squashed Gaussian Mixture by rolling out $\pi_{\text {rand }}$ whereas the second half (reference samModel to promote diverse behaviour. ples) are collected by rolling out $\pi^{*}$. After the objective in Equation 4 is optimized, the discriminator $h_{\psi}$ is discarded Figure 2 illustrates the training process of the proposed and the projection encoding $f_{\chi}(s)$ is extracted to be used expert-guided skill discovery. First, we train the encoder for the objective in Equation 3 Analogous to autoencoders $f_{\chi}$ jointly with the auxiliary classifier $h_{\psi}$ using the external (Hinton \& Salakhutdinov 2006), the idea is that the emdataset $\mathcal{D}$. Secondly, we train the agent using an offline beddings produced by $f_{\chi}(s)$ should now contain a more policy algorithm (SAC), in which the agent samples a skill compact representation of the state space without collapsing $z \sim p(z)$, and then interacts with the environment by taking the dimensions that make "interesting" behaviour stand out. action $a_{t}$ according the skill-conditioned policy $\pi_{\vartheta}\left(a_{t} \mid s_{t}, z\right)$. The environment, then, transits to a new state according to While the use of a reference data changes our approach the transition probability $s_{t+1} \sim p\left(s_{t+1} \mid s_{t}, a_{t}\right)$. We add from a strictly unsupervised skill discovery algorithm, the this transition $\left(s_{z}, z, a_{t}, s_{t+1}\right)$ to the replay buffer $\mathcal{B}$. Simuldiscriminative objective in equation 4 resembles the objectaneously, the policy is updated by sampling a mini-batch tives used in adversarial inverse reinforcement learning (e.g. from the replay buffer $\mathcal{M} \sim \mathcal{B}$, then encoding the next states (Fu et al. 2018) ). However, it differs in that it makes no $e_{t+1}=f_{\chi}\left(s_{t+1}\right)$ and passing them through the discrimiattempts at matching the behaviour of a reference policy nator $q_{\phi}\left(z \mid e_{t+1}\right.$ to get the intrinsic reward. This reward is as it is used only as a prior for simplifying the state space. used by the Q-functions $Q_{\theta}\left(s_{t}, a_{t}, z\right)$ to minimize the soft This approach could also be used with samples from sevBellman residual and update the policy. A pseudocode for eral different reference policies with substantially different the proposed approach can be found in the supplementary marginal state distributions. As long as their variation can material be explained sufficiently without full use of the entire state space, a projection should simplify skill discovery.

\subsection{Implementation}

For learning diverse skills, we use DIAYN as a basis framework. DIAYN uses the Soft Actor-Critic (SAC) algorithm (Haarnoja et al. 2018 ) that is optimized using policy gradient style updates in contrast to the reparameterized version (DDPG style updates (Lillicrap et al.)). They also use a Squashed Gaussian Mixture Model to represent the policy $a \sim \pi_{\vartheta}=\tanh G M M\left(\mu_{\vartheta}(s), \sigma_{\vartheta}(s)\right)$. The learning objective is to maximize the mutual information between the state
and skill $I(S ; Z)$. This objective is optimized by replacing Figure 2: Framework for training expert-guided skill discovery. the task rewards with a pseudo-reward Green arrows show the training of the encoder, red arrows show the agent-environment interaction, while the blue arrows show

$$
r_{z}(s, a) \triangleq \log p_{\phi}(z \mid s)-\log p(z)
$$
the interactions for the offline policy training. Note there is no gradients through the encoder in the offline training. where $q_{\phi}$ is trained to discriminate between skills and $\mathrm{p}(\mathrm{z})$ is the fixed uniform prior over skills (Eysenbach et al. 2018 .

\section{Experiments} A skill is sampled from $z \sim p(z)$ and used throughout a full episode. In our experimental evaluation, we aim to demonstrate the impact of our approach of restricting skill discovery to a In contrast to DIAYN, we use two Q-functions $Q_{\theta}^{1}(s, a)$ projection subspace. We verify our method on both point$\& Q_{\theta}^{2}(s, a)$ where both $\mathrm{Q}$-functions attempt to predict the mazes and continuous control locomotion tasks. All the 



Figure 6: Impact of state-space projection illustrated as distribution of displacements along the locomotion axis used to calculate the extrinsic reward of the environments. By introducing a projection step, the skill search gets focused towards locomotion skills, resulting in a larger spread of displacements.

Table 1: Summary statistics for displacement (along the locomotion axis used to calculate the extrinsic reward of the environments) across the 50 skills learned. The values to the right of $\pm$ indicate standard deviation across 5 seeded runs.

ple 10 trajectories of length 1000 with fairly high returns at the environment's rewards. However, the environment (Ant: $5063.9 \pm 469.8$, Cheetah: $10656.4 \pm 673.5$, Hopper: reward also includes terms for energy expenditure, staying $3348.0 \pm 316.0$ ). The DIAYN algorithm is otherwise identialive (for Ant/Hopper), and collisions (Ant), which would cal to (Eysenbach et al. 2018 ) in terms of hyperparameters; obscure the results. Figure 6 shows the displacement distri$Q, \pi, q_{\phi}$, and $h_{\psi}$ use MLP architectures with 2 hidden laybution of the 50 skills across all runs. The same information ers of width 300 , the entropy bonus weight $\alpha$ is set to $0.1$, is summarized numerically in Table 1 and the number of skills is set to 50 . We limit each skilldiscovery run to $2.5$ million environment interactions but For a qualitative evaluation, we have also composed a video repeat each experiment 5 times with different random seeds with every skill across all runs (including training of SAC agents for $\pi^{*}$ ).

For quantitative evaluation, we look at the displacement

\section{Discussion} along the target locomotion axis for the extrinsic objective. In our approach, we would expect to observe skills that cover For HalfCheetah and Hopper, the runs with state encoding this axis well, i.e., skills that run forward and backward at (+ ENC(3|5)) exhibit a substantially larger spread than the baseline. The best forward-moving cheetah skill moves 178 different speeds. To test this, we roll out each skill deterministically $\left.\right|^{2}$ record its movement over 1000 time steps (or units forward ( $=3311$ environment return), and the best until it reaches a terminal state) and observe the inter-skill backwards-moving cheetah skill moves 186 units backwards spread. A similar assessment is possible by only looking $(=-4025$ environment return $)$. For the hopper environment, the best forward-moving skill manages to jump 20 ${ }^{2}$ Deterministic sampling from our GMM-based policy implies taking the mean of the component with the highest mixture proba- ${ }^{3}$ Video of skills: https://www youtube.com/watch? bility. $\mathrm{V}=\mathrm{X \times} 7$ RVNmv1 $\mathrm{tY}$ units forward, which corresponds to an environment reward of 3268 , which is on the same level as the reference data used to fit its encoder.

The results in the Ant environment are less impressive. There is hardly any difference in how the displacements are distributed for the three approaches, and the total movement is almost negligible. For reference, a good Ant agent trained against the extrinsic reward should obtain displacements in the $100 \mathrm{~s}$ when evaluated over the same trajectory horizon.

Looking at the generated Ant behaviour, we found that the skills produced with encoders typically moved even less than those generated by the baseline. This is not because it
is impossible to generate a linear projection that promotes Figure 7: Embedding weights for a linear projection of the Ant locomotion at various speeds, as the state representation of state space down to $\mathbb{R}^{3}$. Only weights for the first 27 (standardized) all three problems contains a feature for linear velocity along state features are visualized since the remaining 84 are always the target direction. Moreover, the skill classifier does reach zero. The feature that corresponds to (whole body) velocity in the a high accuracy (some breaking $90 \%$ ), so the algorithm mansame direction as the main environment objective is highlighted in orange. ages to find distinguishable skills. We, therefore, suspect that the procedure used to fit the encoder is insufficient for this environment. While it does pick up on linear velocity, it

\section{Conclusion} also picks up on several other features from the state space, which might have made it easier for the algorithm to make In this work, we propose a data-driven approach for guidthe skills distinguishable. ing skill discovery towards learning useful behaviors in complex and high-dimensional spaces. Using examples of To better understand the results of the Ant experiment, we expert data, we fit a state-space projection that preserves investigate the projection matrix learned at the start of the information that makes expert behavior recognizable. The algorithm. Figure 7 gives a representative example of a projection helps discover better behaviors by ensuring that projection learned for an $\mathrm{ENC}(3)$ run. In the diagram, each skills similar to the expert are distinguishable from ranbar indicates the impact each feature of the state space has on domly initialized skills. We show the applicability of our the final embedding. The orange bar highlights the feature approach in a variety of RL tasks, ranging from a simple corresponding to linear torso velocity in the x-direction, i.e. 2 D point maze problem to continuous control locomotion. the direction in which the extrinsic objective rewards an For future work, we aim to improve the embedding scheme agent for running in. All the bars to the left correspond to of the state projection to be suitable for a wider range of joint configurations, link orientations, and all the bars to the environments. right correspond to other velocities.

The feature for velocity in the target direction is well repre- Acknowledgment. We would like to thank Kerstin Bach sented. However, so are the features for the 8 joint velocities and Rudolf Mester for their useful feedback. (8 rightmost bars in each group). Since it is a lot easier to move a single joint than to coordinate all of them for locomotion, the algorithm might more easily converge to this

\section{References} strategy than figure out a way to walk. Moreover, because Agakov, D. B. F. The im algorithm: a variational approach the projection mixes features for movement of single joints with features for locomotion of the entire body, it becomes to information maximization. Advances in neural information processing systems, 16:201, 2004. more difficult for the classifier to distinguish the two. For instance, an ant that figures out how to walk may (in the Brockman, G., Cheung, V., Pettersson, L., Schneider, J., projected space) look similar to one that only twitches some Schulman, J., Tang, J., and Zaremba, W. Openai gym. of its joints. arXiv preprint arXiv: $1606.01540,2016 .$

Campos, V., Trott, A., Xiong, C., Socher, R., Giro-i Nieto, X., and Torres, J. Explore, discover and learn: Unsupervised discovery of state-covering skills. In Interna- tional Conference on Machine Learning, pp. 1317-1327. Oudeyer, P.-Y. and Kaplan, F. What is intrinsic motivation? PMLR, $2020 .$ a typology of computational approaches. Frontiers in neurorobotics, 1:6, $2009 .$ Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diversity is all you need: Learning skills without a reward function. Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. In International Conference on Learning Representations, Curiosity-driven exploration by self-supervised predic$2018 .$ tion. In International Conference on Machine Learning, pp. 2778-2787. PMLR, $2017 .$ Fu, J., Luo, K., and Levine, S. Learning robust rewards with adverserial inverse reinforcement learning. In Interna- Ross, S., Gordon, G. J., and Bagnell, J. A. A reduction of imitation learning and structured prediction to no-regret tional Conference on Learning Representations, $2018 .$ online learning, 2011 . Gregor, K., Rezende, D. J., and Wierstra, D. Variational Salge, C., Glackin, C., and Polani, D. Empowerment-an intrinsic control. arXiv preprint arXiv: 1611.07507, $2016 .$ introduction. In Guided Self-Organization: Inception, pp. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor67-114. Springer, 2014 . critic: Off-policy maximum entropy deep reinforcement Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and learning with a stochastic actor. In International ConKlimov, O. Proximal policy optimization algorithms. ference on Machine Learning, pp. 1861-1870. PMLR, arXiv preprint arXiv: $1707.06347,2017 .$ $2018 .$ Sharma, A., Gu, S., Levine, S., Kumar, V., and Hausman, Hinton, G. E. and Salakhutdinov, R. R. ReducK. Dynamics-aware unsupervised discovery of skills. In ing the Dimensionality of Data with Neural NetInternational Conference on Learning Representations, works. Science, 313(5786):504-507, July 2006 . $2019 .$ ISSN 0036-8075, 1095-9203. doi: $10.1126 /$ science. 1127647. URL https://science.sciencemag Sutton, R. S. and Barto, A. G. Reinforcement learning: An org/content/313/5786/504. Publisher: Ameriintroduction. MIT press, $2018 .$ can Association for the Advancement of Science Section: Report.

Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D., and Kavukcuoglu, K. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv: $1611.05397,2016 .$

Jang, E., Gu, S., and Poole, B. Categorical reparameterization with gumbel-softmax. 112016 .

LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. nature, $521(7553): 436-444,2015$.

Li, K., Gupta, A., Pong, V., Reddy, A., Zhou, A., Yu, J., and Levine, S. Reinforcement learning with bayesian classifiers: Efficient skill learning from outcome examples. Deep RL Workshop, NeurIPS 2020, 2020 .

Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. URL http: / /arxiv.org/abs/1509.02971

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. nature, $518(7540)$ : 529-533, 2015. 

a hyperbolic tangent function, similar to Haarnoja et al. 2018).

3. The policy is updated by climbing the gradient of the minimum of the two Q functions (DDPG-style (Lillicrap et al.)).

$$
J\left(\pi_{\theta}\right)=\min _{i \in\{1,2\}} Q_{\theta}^{i}\left(\pi_{\theta}(a \mid s)\right)+\alpha \mathcal{H}\left(\pi_{\theta}(a \mid s)\right)
$$

This requires that the actions sampled from the policy are differentiable. Each gaussian component of the mixture is reparametrized the standard way, and the mixture is reparametrized with Gumbel-Softmax (Jang et al. 2016).

4. $Q_{\theta}^{1,2}$ is trained by descending on the squared temporal difference (TD) errors generated by the minimum of the target networks $Q_{\theta^{\prime}}^{1} \& Q_{\theta^{\prime}}^{2}$

$$
\begin{aligned}
T D\left(s, a, r, s^{\prime}\right) &=Q_{\theta}^{1,2}(s, a)-r-\gamma(\\
& \min _{i \in\{1,2\}} Q_{\theta^{\prime}}^{i}\left(s^{\prime}, \pi_{\theta}\left(a^{\prime} \mid s^{\prime}\right)\right)+\alpha \mathcal{H}\left(\pi_{\theta}\left(a^{\prime} \mid s^{\prime}\right)\right) \\
)
\end{aligned}
$$

\section{B. Additional Experimental Details}

This appendix extends 5 with additional plots and commentary. Figure $8.10$ show maximum, average and minimum return for the three environments.

\section{Implementation Details}

Conceptually, our skill-discovery algorithm is the same as DIAYN (Eysenbach et al. 2018). There are, however, a few implementation differences that we empirically found to work just as well. Below follows a brief rundown of the key implementation details of the algorithm used in the documented experiments.

1. Two Q-functions $Q_{\theta}^{1}(s, a) \& Q_{\theta}^{2}(s, a)$ are used, both with target clones $Q_{\theta^{\prime}}^{1} \& Q_{\theta^{\prime}}^{2}$ that are continuously updated with polyak averaging. Both Q-functions attempt to predict the same quantity:

$$
\begin{aligned}
Q_{\theta}^{1,2}\left(s_{t}, a_{t}\right) &=\mathbb{E}_{s, a \sim \pi_{\theta}}\left[r\left(s_{t}, a_{t}\right)\right.\\
&\left.+\sum_{t^{\prime}>t} \gamma^{t^{\prime}-t}\left(\alpha \mathcal{H}\left(a_{t^{\prime}}\right)+r\left(s_{t^{\prime}}, a_{t^{\prime}}\right)\right)\right]
\end{aligned}
$$

2. The policy distribution is a mixture of Gaussians with four components. The policy network predicts the mixture logits, as well as the means and log standard deviations of the Gaussians. The output is squashed through 

Figure 8: Maximum, average, and minimum return (computed over all 50 skills) for HalfCheetah-v2 during training. Shaded areas correspond to $\pm$ standard deviation across 5 random seeds.

Figure 9: Maximum, average, and minimum return (computed over all 50 skills) for Hopper-v2 during training. Shaded areas correspond to $\pm$ standard deviation across 5 random seeds.

Figure 10: Maximum, average, and minimum return (computed over all 50 skills) for Ant-v2 during training. Shaded areas correspond to $\pm$ standard deviation across 5 random seeds.